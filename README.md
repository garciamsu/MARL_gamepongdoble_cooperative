# Fundamentos del Aprendizaje por Refuerzo Multiagente (MARL) üß† basados en los principios de los sistemas emergentes üêúüêú

Aprendizaje por Refuerzo Multiagente (MARL) es un √°rea de la inteligencia artificial que se enfoca en c√≥mo m√∫ltiples agentes aut√≥nomos pueden aprender a tomar decisiones a trav√©s de la interacci√≥n con un entorno compartido. Para entender los fundamentos de MARL, comencemos por desglosar los conceptos clave de manera sencilla [X].En MARL, en lugar de un solo agente, tenemos m√∫ltiples agentes que interact√∫an en el mismo entorno; por ejemplo, piensa en un juego de f√∫tbol. Cada jugador (agente) necesita aprender a cooperar con sus compa√±eros y competir contra el equipo contrario para ganar el partido. Estos agentes pueden:

- Cooperar: Trabajar juntos para lograr un objetivo com√∫n.
- Competir: Intentar superar a los dem√°s para lograr su propio objetivo.
- Mixto: Una combinaci√≥n de cooperaci√≥n y competencia.

 El Aprendizaje por Refuerzo (RL) es una t√©cnica donde un agente aprende a tomar decisiones mediante ensayo y error, buscando maximizar una recompensa acumulada. Imagina a un ni√±o aprendiendo a montar en bicicleta: prueba diferentes movimientos, se cae algunas veces, pero con el tiempo aprende a mantener el equilibrio y avanzar sin caerse.

- Agente: Es el aprendiz o el que toma decisiones.
- Entorno: Es el mundo con el que interact√∫a el agente.
- Acciones: Son las decisiones o movimientos que el agente puede realizar.
- Recompensas: Son se√±ales que indican qu√© tan bien lo est√° haciendo el agente.
- Pol√≠tica: Es la estrategia que sigue el agente para decidir qu√© acci√≥n tomar en cada situaci√≥n.

Por otra parte, los sistemas emergentes se caracterizan por comportamientos complejos que surgen de interacciones simples entre componentes individuales. Al aplicar estos principios al aprendizaje por refuerzo multiagente, consideramos lo siguiente:

- **Interacciones Locales:** Los agentes interact√∫an principalmente con su entorno inmediato y, de forma limitada, con otros agentes.
- **Reglas Simples:** Los agentes siguen reglas sencillas que, al combinarse, generan comportamientos complejos.
- **Descentralizaci√≥n:** No existe un controlador central; la coordinaci√≥n surge de las interacciones entre agentes.
- **Adaptaci√≥n y Aprendizaje:** Los agentes se adaptan y aprenden de sus experiencias, lo que conduce a la evoluci√≥n del comportamiento del sistema.
- **Autoorganizaci√≥n:** El sistema se organiza a s√≠ mismo en patrones o comportamientos sin gu√≠a externa.

## Desaf√≠os Clave en MARL üöß

El aprendizaje con m√∫ltiples agentes introduce varios desaf√≠os adicionales:

### No Estacionariedad del Entorno
En RL tradicional, el entorno es generalmente fijo. En MARL, el entorno cambia constantemente porque otros agentes tambi√©n est√°n aprendiendo y cambiando sus estrategias. Lo que implica que los agentes deben adaptarse no solo al entorno, sino tambi√©n a las acciones y aprendizajes de los otros agentes.

### Coordinaci√≥n y Cooperaci√≥n
Lograr que los agentes trabajen juntos eficazmente puede ser dif√≠cil, especialmente si no pueden comunicarse directamente. Esto implica que los agentes deben desarrollar estrategias para coordinar sus acciones y maximizar las recompensas compartidas.

**La cooperaci√≥n** se refiere al proceso en el que dos o m√°s agentes trabajan juntos voluntariamente hacia un objetivo com√∫n o compartido. En la cooperaci√≥n, los agentes pueden estar dispuestos a sacrificar sus propios intereses individuales o recursos para beneficiar al grupo o para lograr una meta colectiva. Ejemplo: Ajustar la velocidad para evitar chocar con otros veh√≠culos. Caracter√≠sticas clave de la cooperaci√≥n:

- Objetivo compartido: Los agentes tienen metas comunes y trabajan para alcanzarlas juntos.
- Beneficio mutuo: Las acciones de los agentes est√°n orientadas a beneficiar al grupo, no solo a s√≠ mismos.
- Comunicaci√≥n y entendimiento mutuo: Puede requerir que los agentes compartan informaci√≥n y comprendan las intenciones de los dem√°s.
- Sacrificio personal: Los agentes pueden renunciar a ventajas individuales por el bien del grupo.

Por otra parte, **La coordinaci√≥n** implica la organizaci√≥n de acciones o esfuerzos entre dos o m√°s agentes para asegurar que trabajen juntos de manera eficiente y sin conflictos. La coordinaci√≥n se enfoca en sincronizar y armonizar las acciones para lograr un resultado √≥ptimo, evitando interferencias o duplicaci√≥n de esfuerzos. A diferencia de la cooperaci√≥n, la coordinaci√≥n no necesariamente requiere que los agentes compartan un objetivo com√∫n o que est√©n dispuestos a sacrificar sus propios intereses. Ejemplo: Detenerse para permitir que una ambulancia pase r√°pidamente. Caracter√≠sticas clave de la coordinaci√≥n:

- Sincronizaci√≥n de acciones: Los agentes alinean sus actividades en tiempo y forma.
- Prevenci√≥n de conflictos: Se evitan interferencias o colisiones entre las acciones de los agentes.
- Independencia de objetivos: Los agentes pueden tener metas individuales distintas, pero a√∫n as√≠ necesitan coordinarse.
- Comunicaci√≥n m√≠nima: A veces, la coordinaci√≥n puede lograrse con informaci√≥n limitada o se√±ales indirectas.

Comprender estas diferencias es crucial en campos como la inteligencia artificial y la rob√≥tica, donde dise√±ar sistemas que puedan coordinarse y cooperar efectivamente puede llevar a soluciones m√°s eficientes y armoniosas en entornos multiagente.

### Asignaci√≥n de Cr√©ditos
Determinar c√≥mo las acciones individuales de un agente contribuyen al resultado global. Lo que es dif√≠cil saber qu√© agente merece cr√©dito por una recompensa obtenida en equipo.

## Fundamentos de MARL üß©

**Agentes y Entorno**
- Cada agente tiene su propia percepci√≥n del entorno, que puede ser completa o limitada.
- Los agentes toman acciones basadas en su percepci√≥n y pol√≠tica.
- El entorno responde a las acciones de todos los agentes, proporcionando nuevas observaciones y recompensas.

**Pol√≠ticas y Aprendizaje**
- Una pol√≠tica es la estrategia que un agente sigue para decidir sus acciones.
- Los agentes usan algoritmos de aprendizaje, como Q-learning, para actualizar sus pol√≠ticas basados en las recompensas recibidas.
- En MARL, los agentes pueden aprender de manera independiente o conjunta.

**Tipos de Interacciones entre Agentes**
- Cooperativo: Todos los agentes comparten el mismo objetivo.
- Competitivo: Los agentes tienen objetivos opuestos.
- Mixto: Combina elementos cooperativos y competitivos.

## Algoritmos B√°sicos en MARL üìö
Algunos algoritmos comunes utilizados en MARL incluyen:

- Q-learning Independiente
Cada agente aplica Q-learning por su cuenta, sin considerar las acciones de los dem√°s. Tiene como ventaja su Simplicidad pero  puede ser ineficiente debido a la no estacionariedad del entorno.
- Aprendizaje Conjunto: Los agentes aprenden una pol√≠tica conjunta que considera las acciones de todos. Tiene como ventaja que Mejora su coordinaci√≥n, pero el espacio de estados y acciones se vuelve exponencialmente grande con m√°s agentes.
- M√©todos Basados en Valor y Pol√≠tica. En el caso de los M√©todos de Valor, los agentes estiman el valor de los estados o acciones; que con los M√©todos de Pol√≠tica, los agentes aprenden directamente una pol√≠tica sin estimar valores.

# Caso de estudio
## Juego de Ping Pong doble cooperativo
### Descripci√≥n del juego

El Juego de Ping Pong Doble Cooperativo es una variante del tenis de mesa tradicional en la que dos jugadores trabajan juntos  en equipo (mismo lado) para mantener la pelota en juego el mayor tiempo posible. A diferencia del juego competitivo, donde los jugadores intentan vencer al oponente, en esta modalidad cooperativa el objetivo es colaborar para lograr una meta com√∫n, es decir, lograr el mayor n√∫mero posible de golpes consecutivos, promoviendo la coordinaci√≥n y el trabajo en equipo.

![](files/assets/ping_pong_game.png)

### Relaci√≥n del juego de ping pong cooperativo, los elementos claves de MARL y sistemas emergentes
A continuaci√≥n, reestructuraremos el enfoque anterior para implementar Multi-Agent Reinforcement Learning (MARL) en el juego Pong con dos jugadores que cooperan y coordinan, tomando en cuenta los principios de los sistemas emergentes. Nos centraremos en c√≥mo interacciones simples y locales entre agentes pueden conducir a comportamientos complejos y coordinados sin necesidad de control centralizado.

||**Definici√≥n**|**Ejemplo**|
| :- | :-: | :-: |
|Acciones (Actions) **(A)**|Las acciones son las decisiones o movimientos que un agente puede realizar en un entorno dado.|El jugador puede tomar dos acciones: mover la pala hacia arriba o hacia abajo. Estas acciones impactan directamente en la posici√≥n del jugador y, por lo tanto, en la capacidad de devolver la pelota.|
|Pol√≠tica (Policy)**(œÄ)**|La pol√≠tica es la estrategia o regla que el agente sigue para decidir qu√© acci√≥n tomar en un estado dado.|Una pol√≠tica simple podr√≠a ser: "Si la pelota est√° por encima de la pala, mueve la pala hacia arriba; si est√° por debajo, mueve la pala hacia abajo". En t√©rminos de RL, una pol√≠tica determina c√≥mo el jugador act√∫a en funci√≥n de la posici√≥n actual de la pelota.|
|Funci√≥n de Valor (Value Function)**(Q)**|La funci√≥n de valor estima la utilidad a largo plazo de un estado, es decir, el valor esperado del retorno futuro que puede recibir el agente desde un estado particular.|El valor de un estado podr√≠a representar la probabilidad de que la pelota sea devuelta exitosamente y se mantenga en juego durante m√°s tiempo, dado que el jugador act√∫a √≥ptimamente desde esa posici√≥n.Por ejemplo, si la pelota est√° cerca del borde superior de la pantalla, el estado podr√≠a tener un valor m√°s bajo porque es m√°s dif√≠cil devolver la pelota en esa posici√≥n.|
|Funci√≥n de Recompensa (Reward Function):**( R )**|La funci√≥n de recompensa proporciona feedback inmediato sobre la bondad de una acci√≥n en un estado dado, generalmente en forma de una se√±al num√©rica.|Una recompensa positiva se podr√≠a otorgar cada vez que el jugador devuelve la pelota con √©xito (por ejemplo, +1 por cada rebote exitoso).Una recompensa negativa podr√≠a ser otorgada si la pelota pasa m√°s all√° de la pala y el jugador pierde (-1 por perder).La recompensa gu√≠a al agente (jugador) a maximizar la cantidad de rebotes y, por lo tanto, mejorar su estrategia de juego.|
|Modelo **(Model)**:|Un modelo en RL predice c√≥mo se va a comportar el entorno en respuesta a las acciones del agente, es decir, predice las transiciones entre estados y las recompensas asociadas.| El modelo podr√≠a predecir c√≥mo cambiar√° la posici√≥n de la pelota despu√©s de que el jugador mueva la pala hacia arriba o hacia abajo. Aunque en un entorno RL puro, el agente aprender√≠a estas transiciones a trav√©s de la interacci√≥n, en este juego se asume que las reglas f√≠sicas de la pelota son conocidas (como la reflexi√≥n de la pelota en los bordes o la colisi√≥n con la pala).|

En un contexto de aprendizaje por refuerzo, un agente (en este caso, el jugador) aprender√≠a a jugar al ping pong optimizando una pol√≠tica que maximiza las recompensas acumuladas (por ejemplo, mantener la pelota en juego durante el mayor tiempo posible) al tomar acciones (mover la pala) basadas en la funci√≥n de valor que eval√∫a la probabilidad de √©xito futuro desde un estado dado. El modelo puede estar impl√≠cito en el entorno, prediciendo c√≥mo la pelota reacciona a las acciones del jugador. Este marco de RL podr√≠a aplicarse para desarrollar un agente aut√≥nomo capaz de jugar al ping pong aprendiendo a trav√©s de la experiencia, sin necesidad de reglas predefinidas.

<img src="files/assets/componentes.png" width="100%"/>

### Dise√±o de un sistema MARL basados en principios emergentes
1. Definici√≥n del Entorno y los Agentes
**Entorno:** El juego Pong con dos paletas controladas por agentes y una pelota.
**Agentes:** Dos jugadores (paletas) que act√∫an de forma aut√≥noma y descentralizada.
**Percepci√≥n Local:** Cada agente percibe solo informaci√≥n local:
- Su propia posici√≥n.
- La posici√≥n y velocidad de la pelota relativa a s√≠ mismo.
- Opcionalmente, informaci√≥n limitada sobre el otro agente (por ejemplo, su posici√≥n relativa).

2. Dise√±o de Reglas Simples y Acciones
**Acciones Disponibles:**
- Mover hacia arriba.
- Mover hacia abajo.
- Quedarse quieto.

**Pol√≠tica de Acci√≥n:** Los agentes utilizan una pol√≠tica epsilon-greedy basada en su percepci√≥n local para seleccionar acciones.

3. Aprendizaje Descentralizado e Independiente: A traves del algoritmo Q-Learning Independiente, cada agente mantiene su propia tabla Q y actualiza sus valores bas√°ndose en sus experiencias individuales.

**Recompensas Locales:**
- Positivas cuando el agente devuelve la pelota exitosamente.
- Negativas cuando falla al devolver la pelota.
- Objetivo Compartido: Mantener la pelota en juego el mayor tiempo posible, lo que incentiva la cooperaci√≥n emergente.

4. Interacciones entre Agentes
- Coordinaci√≥n Indirecta: Los agentes influyen en el comportamiento del otro a trav√©s del entorno (la pelota).
- Comunicaci√≥n Limitada (Opcional): Se puede permitir una comunicaci√≥n m√≠nima para mejorar la coordinaci√≥n sin centralizar el control.

5. Autoorganizaci√≥n y Comportamiento Emergente
- Adaptaci√≥n Mutua: Los agentes ajustan sus pol√≠ticas bas√°ndose en las acciones observadas del otro agente.
- Desarrollo de Estrategias: A trav√©s del aprendizaje, pueden surgir estrategias como posicionamiento √≥ptimo o roles complementarios.

### Pasos Detallados de Implementaci√≥n

**Paso 1:** Configuraci√≥n del Entorno: Corresponde al espacio de Estados Locales, para cada agente incluye:
- Posici√≥n de la paleta (y_agente).
- Posici√≥n y velocidad de la pelota relativa al agente (delta_x_pelota, delta_y_pelota, v_x_pelota, v_y_pelota).

> Nota: Para reducir la complejidad se puede aplicar la discretizaci√≥n que consiste en convertir valores continuos en categor√≠as discretas

**Paso 2:** Definici√≥n de las Acciones y Pol√≠ticas
* Acciones:
    - Arriba, Abajo, Quieto.
    - Pol√≠tica Epsilon-Greedy:
* Con probabilidad epsilon, el agente explora seleccionando una acci√≥n aleatoria.
* Con probabilidad 1 - epsilon, explota seleccionando la acci√≥n con el mayor valor Q.

**Paso 3:** Estructura de Recompensas Locales, pero tambien se podria agregar opcionalmente, una peque√±a penalizaci√≥n por cada movimiento para incentivar la eficiencia.
- +1 por devolver la pelota exitosamente.
- -1 por fallar al devolver la pelota.

**Paso 4:** Algoritmo de Q-Learning Descentralizado, para cada agente y en cada paso de tiempo:

1. Observaci√≥n del Estado Local s.
2. Selecci√≥n de Acci√≥n a: Usando la pol√≠tica epsilon-greedy.
3. Ejecuci√≥n de la Acci√≥n a: Actualizar la posici√≥n de la paleta seg√∫n la acci√≥n.
4. Actualizaci√≥n del Entorno: El entorno actualiza la posici√≥n y velocidad de la pelota.
5. Recepci√≥n de Recompensa r: Basada en el resultado de la acci√≥n (√©xito o fallo al devolver la pelota).
6. Observaci√≥n del Nuevo Estado s'.
7. Actualizaci√≥n de la Tabla Q:

   ![](files/assets/Qlearning.png)

   Donde:
   1. Q(s,a) es el valor Q para el estado actual s y la acci√≥n a.
   1. Œ± es la tasa de aprendizaje (learning rate).
   1. r es la recompensa obtenida despu√©s de tomar la acci√≥n a.
   1. Œ≥ es el factor de descuento, que determina la importancia de las recompensas futuras.
   1. ![](Aspose.Words.4016bc56-786a-4dd4-8df8-0de9cfbe8fd3.004.png)es el valor Q m√°ximo para el siguiente estado s‚Ä≤ y todas las acciones posibles a‚Ä≤.

**Paso 5:** Emergencia de la Coordinaci√≥n
* Influencia Mutua a trav√©s del Entorno:
    - Las acciones de un agente afectan indirectamente las experiencias del otro.
* Aprendizaje Adaptativo:
    - Los agentes aprenden a predecir el comportamiento de la pelota y del otro agente, ajustando sus acciones en consecuencia.
* Comportamientos Emergentes:
    - Sin programaci√≥n expl√≠cita, pueden surgir patrones de juego cooperativo.

**Paso 6:** Evaluaci√≥n y Mejora
* M√©tricas de Rendimiento:
    - Tiempo promedio que la pelota permanece en juego.
* Frecuencia de devoluciones exitosas.
    - Ajuste de Par√°metros:
    - Ajustar \alpha, \gamma y epsilon para mejorar la convergencia y el rendimiento.
* Introducci√≥n de Variabilidad:
    - Modificar condiciones del juego para fomentar la adaptabilidad (por ejemplo, cambiar la velocidad de la pelota).

## Herramientas utiles
- Convertir WORD a Markdown Online [Aspose](https://products.aspose.app/words/conversion/word-to-md)

## Bibliograf√≠a

- [1] Y. -C. Choi and H. -S. Ahn, "A survey on multi-agent reinforcement learning: Coordination problems," *Proceedings of 2010 IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications*, QingDao, China, 2010, pp. 81-86
- [2] Rana, A.S., Iqbal, F., Siddiqui, A.S. and Thomas, M.S. (2019), Hybrid methodology to analyse reliability and techno-economic evaluation of microgrid configurations. IET Gener. Transm. Distrib., 13: 4778-4787.
- [3] Sutton, Richard S., and Andrew G. Barto. *Reinforcement Learning: An Introduction*. MIT Press, 2018. 
- [4] Ris-Ala, R. (2023). *Fundamentals of Reinforcement Learning*. Springer Nature Switzerland.
